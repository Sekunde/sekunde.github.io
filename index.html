<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Ji Hou</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ji Hou (侯骥)</name>
		      
              </p>
              <p>I am a Research Scientist at Meta Generative AI. Previously, I did my Ph.D. at TUM Visual Computing Group headed by <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Prof. Matthias Niessner</a>, where I work on Computer Vision and 3D Scene Understanding. 
		During my Ph.D., I did an internship at Facebook AI Research (FAIR) with <a href="https://www.sainingxie.com/">Saining Xie</a> and <a href="https://ai.facebook.com/people/benjamin-graham/">Benjamin Graham</a> on 3D representation and data-efficient learning.
                 Before that, I obtained my master at RWTH Computer Vision Group headed by <a href="https://www.vision.rwth-aachen.de/person/1/">Prof. Bastian Leibe</a>, where I studied on Computer Vision and Machine Learning. 
              </p>
              <p>
               I am interested in research and applications on 3D Computer Vision, e.g. 3D Reconstruction, VR/AR, robotics and autonomous driving etc.
              </p>
              <p style="text-align:center">
			      	<a href="mailto:ji.hou@tum.de"><i class="fa fa-envelope" style="font-size:20px" target="_blank"></i>&nbsp; Email</a> &nbsp;/&nbsp;
			      	<a href="https://scholar.google.de/citations?hl=en&user=63TZXq4AAAAJ" target="_blank"><i class="ai ai-google-scholar-square ai-3x" style="font-size:20px"></i>&nbsp; Google Scholar</a> &nbsp;/&nbsp;
			      	<a href="https://github.com/sekunde/" target="_blank"><i class="fa fa-github" style="font-size:20px"></i>&nbsp; Github</a> &nbsp;/&nbsp;
		     	 	<a href="https://twitter.com/sekunde_" target="_blank"><i class="fa fa-twitter" style="font-size:20px"></i>&nbsp; Twitter</a> &nbsp;/&nbsp;
		      		<a href="https://www.linkedin.com/in/ji-hou-3b64a2a0/" target="_blank"><i class="fa fa-linkedin" style="font-size:20px"></i>&nbsp; Linkedin</a> 
			      	</p>
              
           </td>		  
		  
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:80%;max-width:80%;border-radius:50%" alt="Avatar" src="images/icon.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
	      
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <!-- <strong> Our team is hiring several interns based in Zurich or U.S. for Summer 2023 </strong>. If you have strong research background in 3D Scene Understanding, 3D Representation Learning or 3D Data-Efficient Learning, or know any such candidates, please send me an email!	 --> 
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nerf_det.jpg" alt="nerf_det" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="" target="_blank">
                <papertitle>NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection</papertitle>
              </a>
              <br>
          <a href="https://chenfengx.com/" target="_blank">Chenfeng Xu</a>,
              <a href="https://scholar.google.com/citations?user=K3QJPdMAAAAJ&hl=en" target="_blank">Bichen Wu</a>,
		<strong>Ji Hou</strong>	&#42;,
		<a href="https://scholar.google.com/citations?user=JdE_LFYAAAAJ&hl=zh-CN" target="_blank">Sam Tsai</a>,
                 <a href="https://www.liruilong.cn/" target="_blank">Ruilong Li</a>,
              <a href="https://scholar.harvard.edu/jwang/home" target="_blank">Jialiang Wang</a>,
		    <a href="https://zhanwei.site/" target="_blank">Wei Zhan</a>
		     <a href="https://scholar.google.com/citations?user=G03EzSMAAAAJ&hl=en" target="_blank">Zijian He</a>
		    <a href="https://sites.google.com/site/vajdap" target="_blank">Peter Vajda</a>
		    <a href="https://people.eecs.berkeley.edu/~keutzer/" target="_blank">Kurt Keutzer</a>
		    <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/" target="_blank">Masayoshi Tomizuka</a>
	
              <br>
		    <em>International Conference on Computer Vision (<strong>ICCV</strong>), 2023 </em>
              <br>
             <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_NeRF-Det_Learning_Geometry-Aware_Volumetric_Representation_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf" target="_blank">paper</a> /
             <a href="data/bib/nerf_det.txt" target="_blank">bibtex</a> /
 		<a href="https://chenfengxu714.github.io/nerfdet/" target="_blank">project</a> /
	    <a href='https://github.com/facebookresearch/NeRF-Det' target="_blank">code</a> 
		<br> <small>&#42; Corresponding author. </small> 
              <p>NeRF-Det is a novel method for 3D detection with posed RGB images as input. Our method makes novel use of NeRF in an end-to-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance.</p>
		 
            </td>
          </tr>
		
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/RoITr.jpg" alt="RoITr" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="" target="_blank">
                <papertitle>Rotation-Invariant Transformer for Point Cloud Matching</papertitle>
              </a>
              <br>
          <a href="https://scholar.google.com/citations?user=g7JfRn4AAAAJ&hl=en" target="_blank">Hao Yu</a>,
              <a href="https://scholar.google.com/citations?user=DnHBAN0AAAAJ&hl=en" target="_blank">Zheng Qin</a>,
		    <strong>Ji Hou</strong>,
		<a href="https://www.cs.cit.tum.de/camp/members/mahdi-saleh/" target="_blank">Mahdi Saleh</a>,
                 <a href="" target="_blank">Dongsheng Li</a>,
              <a href="https://campar.in.tum.de/Main/BenjaminBusam.html" target="_blank">Benjamin Busam</a>,
		    <a href="https://campar.in.tum.de/Main/SlobodanIlic" target="_blank">Slobodan Ilic</a>
              <br>
		    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </em>
              <br>
             <a href="https://arxiv.org/pdf/2303.08231.pdf" target="_blank">paper</a> /
             <a href="data/bib/RoITr.txt" target="_blank">bibtex</a> /
	    <a href='https://github.com/haoyu94/RoITr' target="_blank">code</a> 
              <p>We introduce RoITr, a Rotation-Invariant Transformer to cope with the pose variations in the point cloud matching task. On the challenging 3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5 percentage points in terms of the Inlier Ratio and the Registration Recall, respectively</p>
            </td>
          </tr>
		
		
	
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mask3d.jpg" alt="mask3d" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="" target="_blank">
                <papertitle>Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors</papertitle>
              </a>
              <br>
          
		<strong>Ji Hou</strong>,
              <a href="https://sites.google.com/view/xiaoliangdai/" target="_blank">Xiaoliang Dai</a>,
		<a href="https://research.facebook.com/people/he-zijian/" target="_blank">Zijian He</a>,
                 <a href="https://www.3dunderstanding.org/team.html" target="_blank">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
              <br>
		    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </em>
              <br>
              <a href="https://arxiv.org/pdf/2302.14746.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=s0ITbsdekl4" target="_blank">video</a> /
              <a href="data/bib/mask3d.txt" target="_blank">bibtex</a> 
              <p>We demonstrate the Mask3D is particularly effective in embedding 3D priors into the powerful 2D ViT backbone, enabling improved representation learning for various scene understanding tasks, such as semantic segmentation, instance segmentation and object detection.</p>
            </td>
          </tr>
		
		
		

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pcr_cg.jpg" alt="pcr_cg" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700439.pdf" target="_blank">
                <papertitle>PCR-CG: Point Cloud Registration via Deep Explicit Color and Geometry</papertitle>
              </a>
              <br>
          
		<a href="" target="_blank">Yu Zhang</a>,
              <a href="" target="_blank">Junle Yu</a>,
		<a href="https://imr.sjtu.edu.cn/en/po_adj/576.html" target="_blank">Xiaolin Huang</a>,
		    <a href="https://scholar.google.com/citations?user=WsRH-pYAAAAJ&hl=zh-CN" target="_blank">Wenhui Zhou</a>,
                  <strong>Ji Hou</strong>
              <br>
		    <em>European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700439.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=gYbgxEV9RHI" target="_blank">video</a> /
              <a href="data/bib/pcr_cg.txt" target="_blank">bibtex</a> /
		    <a href="data/bib/6316.pdf" target="_blank">poster</a> /
	      <a href='https://github.com/Gardlin/PCR-CG' target="_blank">code</a>
              <p>We introduce PCR-CG: a novel 3D point cloud registration module explicitly embedding the color signals into geometry representation. With our designed 2D-3D projection module, the pixel features in a square region centered at correspondences perceived from images are effectively correlated with point cloud representations.</p>
            </td>
          </tr>
		
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pri3d.jpg" alt="pri3d" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2104.11225.pdf" target="_blank">
                <papertitle>Pri3D: Can 3D Priors Help 2D Representation Learning?</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
		<a href="https://vcl.ucsd.edu/~sxie/about/" target="_blank">Saining Xie</a>,
              <a href="https://ai.facebook.com/people/benjamin-graham/" target="_blank">Benjamin Graham</a>,
		<a href="https://www.3dunderstanding.org/team.html" target="_blank">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
              <br>
		    <em>International Conference on Computer Vision (<strong>ICCV</strong>), 2021 </em>
              <br>
              <a href="https://arxiv.org/pdf/2104.11225.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=S2VodtyfQbQ" target="_blank">video</a> /
              <a href="data/bib/pri3d.txt" target="_blank">bibtex</a> /
		    <a href="http://niessnerlab.org/projects/hou2021pri3d.html" target="_blank">project</a> /
	      <a href='https://github.com/Sekunde/Pri3D' target="_blank">code</a>
              <p>Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints.</p>
            </td>
          </tr>
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/panoptic3d.jpg" alt="panoptic3d" width="200" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2111.02444.pdf">
                <papertitle>Panoptic 3D Scene Reconstruction From a Single RGB Image</papertitle>
              </a>
              <br>
		<a href="https://manuel-dahnert.com/">Manuel Dahnert</a>,
              <strong>Ji Hou</strong>,
		
		<a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
		<a href="https://www.3dunderstanding.org/team.html" target="_blank">Angela Dai</a>
              
              <br>
		    <em>Advances in Neural Information Processing System (<strong>NeurIPS</strong>), 2021 </em>
              <br>
               <a href="https://arxiv.org/pdf/2111.02444.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=YVxRNHmd5SA" target="_blank">video</a> /
              <a href="data/bib/panoptic3d.txt" target="_blank">bibtex</a> /
		 <a href="https://manuel-dahnert.com/research/panoptic-reconstruction" target="_blank">project</a> /
	      <a href='https://github.com/xheon/panoptic-reconstruction' target="_blank">code</a>
		    
              <p>Inspired by 2D panoptic segmentation, we propose to unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation into the task of panoptic 3D scene reconstruction -- from a single RGB image, predicting the complete geometric reconstruction of the scene in the camera frustum of the image, along with semantic and instance segmentations.</p>
            </td>
          </tr>
		
		
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/csc.jpg" alt="3dsis" width="185" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2012.09165.pdf" target="_blank">
                <papertitle>Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="https://ai.facebook.com/people/benjamin-graham/" target="_blank">Benjamin Graham</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>,
	      <a href="https://vcl.ucsd.edu/~sxie/about/" target="_blank">Saining Xie</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021 </em><br>
              <font color="red">(Oral Presentation)</font>
              <br>
              <a href="https://arxiv.org/pdf/2012.09165.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=E70xToZLgs4" target="_blank">video</a> /
              <a href="data/bib/csc.txt" target="_blank">bibtex</a> /
              <a href='https://sekunde.github.io/project_efficient/' target="_blank">project</a> /
	      <a href='https://github.com/facebookresearch/ContrastiveSceneContexts' target="_blank">code</a> /
		    <a href='http://kaldir.vc.in.tum.de/scannet_benchmark/data_efficient/' target="_blank">benchmark</a>
              <p>Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmentation) of the baseline performance that uses full annotations.</p>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rfd.jpg" alt="3dsis" width="185" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2011.14744.pdf" target="_blank">
                <papertitle>RfD-Net: Point Scene Understanding by Semantic Instance Reconstruction</papertitle>
              </a>
              <br>
              <a href="https://yinyunie.github.io/" target="_blank">Yinyu Nie</a>,
	      <strong>Ji Hou</strong>,
	      <a href="https://mypage.cuhk.edu.cn/academics/hanxiaoguang/index.html" target="_blank">Xiaoguang Han</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021 </em> <br>
              <a href="https://arxiv.org/pdf/2011.14744.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=RHHFC2UaZtQ" target="_blank">video</a> /
              <a href="data/bib/rfd.txt" target="_blank">bibtex</a> /
	      <a href="https://github.com/yinyunie/RfDNet" target="_blank">code</a>
              <p>In this work, we introduce RfD-Net that jointly detects and reconstructs dense object surfaces directly from raw point clouds. Instead of representing scenes with regular grids, our method leverages the sparsity of point cloud data and focuses on predicting shapes that are recognized with high objectness.</p>
            </td>
          </tr>

		
		
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/revealnet.png" alt="revealnet" width="160" style="border-radius:15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1904.12012.pdf" target="_blank">
                <papertitle>RevealNet: Seeing Behind Objects in RGB-D Scans</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="https://www.3dunderstanding.org/team.html" target="_blank">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020 </em>
              <br>
              <a href="https://arxiv.org/pdf/1904.12012.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=iyT_fkOA2yg" target="_blank">video</a> /
              <a href="data/bib/revealnet.txt" target="_blank">bibtex</a> /
              <a href='http://niessnerlab.org/projects/hou2020revealnet.html' target="_blank">project</a>
              <p>This paper introduces the task of semantic instance completion: from an incomplete, 
                RGB-D scan of a scene, we detect the individual object instances comprising the scene and jointly infer their complete object geometry.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/3dsis.png" alt="3dsis" width="160" style="border-radius: 15px">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1812.07003.pdf" target="_blank">
                <papertitle>3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</papertitle>
              </a>
              <br>
              <strong>Ji Hou</strong>,
              <a href="https://www.3dunderstanding.org/team.html" target="_blank">Angela Dai</a>,
              <a href="http://niessnerlab.org/members/matthias_niessner/profile.html" target="_blank">Matthias Nießner</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019 </em> <br>
              <font color="red">(Oral Presentation)</font>
              <br>
              <a href="https://arxiv.org/pdf/1812.07003.pdf" target="_blank">paper</a> /
              <a href="https://www.youtube.com/watch?v=IH9rNLD1-JE" target="_blank">video</a> /
              <a href="data/bib/3dsis.txt" target="_blank">bibtex</a> /
              <a href='http://niessnerlab.org/projects/hou2019sis.html' target="_blank">project</a> /
              <a href='https://github.com/Sekunde/3D-SIS' target="_blank">code</a>
              <p>We introduce 3D-SIS, a novel neural network architecture for 3D semantic instance segmentation in commodity RGB-D scans. The core idea of our method is to jointly learn from both geometric and color signal, thus enabling accurate instance predictions.</p>
            </td>
          </tr>

        </tbody></table>
        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
		<td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/teaching.png" width="160" style="border-radius:15px">
                </td>
                <td width="75%" valign="center" style="line-height:30px;">
                   <a target="_blank" href="https://www.3dunderstanding.org/teaching.html">Teaching
                    Assistant, Seminar for 3D Machine Learning  - Summer 2021</a>
		  <br>
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/adl4cv-ws19/">Teaching
                    Assistant, Advanced Deep Learning for Computer Vision - Winter 2019/20</a>
                
                  <a target="_blank" href="https://dvl.in.tum.de/teaching/i2dl-ss18/">Teaching
                    Assistant, Introdcution to Deep Learning - Summer 2018</a>
			<br>
                  <a target="_blank" href="https://vision.cs.tum.edu/teaching/ws2017/dl4cv">Teaching
                    Assistant, Deep Learning for Computer Vision - Winter 2017/18</a>
                </td>
              </tr>
            </tbody>
          </table>
	      
	      
	           
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>	Review Experiences </heading>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="75%" valign="center" style="line-height:30px;">
                 IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
			 <br>
		 ACM Transactions on Multimedia Computing Communications and Applications (TOMM)
			 <br>
		 International Journal of Computer Vision (IJCV)
			 <br>
		 ISPRS Journal of Photogrammetry and Remote Sensing
			 <br>
		 IEEE Robotics and Automation Letters (RA-L)
			 <br>
		 Pattern Recognition Letters
			 <br>
		 Neurocomputing
			 <br>
		 Computers & Graphics
			 <br>
		 Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH)
			 <br>
		International Conference on Learning Representations (ICLR)
			 <br>
		Association for the Advancement of Artificial Intelligence (AAAI)
			<br>
		  Conference on Computer Vision and Pattern Recognition (CVPR)
			<br>
		International Conference on Computer Vision (ICCV) 
			<br>
		International Conference on Robotics and Automation (ICRA)
			<br>
			European Conference on Computer Vision (ECCV)
			<br>
			Neural Information Processing Systems (NeurIPS)
			<br>
			International Joint Conference on Artificial Intelligence (IJCAI)
		 </td>
              </tr>

            </tbody>
          </table>
	      
	      
	       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>	Organizer </heading>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="75%" valign="center" style="line-height:30px;">
                The 2nd Computer Vision for Metaverse Workshop at ICCV 2023
			
		 </td>
              </tr>

            </tbody>
          </table>
	      
	      
	     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		    <heading>Visiting Map </heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="100%" height="350"  valign="center" style="line-height:20px;">
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=77736d&w=700&t=tt&d=GvT-QjpV5PqufpwHmep1052Qgr-C31__78lRz1WwjcE&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5e1717"></script>		</td>      
              </tr>
            </tbody>
          </table>    
	      
	      
  
        
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
      Credits: <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">Jon Barron</a>
          </font>
        </p>
        </td>
      </tr>
      </tbody></table>
	      

</body>

</html>
